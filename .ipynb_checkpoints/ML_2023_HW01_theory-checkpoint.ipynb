{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b4cc74",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda88da5",
   "metadata": {},
   "source": [
    "In this part of the homework, you are to solve several theoretical problems related to machine learning algorithms.\n",
    "\n",
    "* For every separate problem you can get **INTERMEDIATE scores**.\n",
    "\n",
    "\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "\n",
    "\n",
    "* You must write your solution for each problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "\n",
    "## $\\LaTeX$ in Jupyter\n",
    "\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "\n",
    "* to write **cases of equations** use \n",
    "```markdown\n",
    "$$ left-hand-side = \\begin{cases}\n",
    "                     right-hand-side on line 1, & \\text{condition} \\\\\n",
    "                     right-hand-side on line 2, & \\text{condition} \\\\\n",
    "                    \\end{cases} $$\n",
    "```\n",
    "\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "$$ \\begin{align}\n",
    "    left-hand-side on line 1 &= right-hand-side on line 1 \\\\\n",
    "    left-hand-side on line 2 &= right-hand-side on line 2\n",
    "   \\end{align} $$\n",
    "```\n",
    "\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518781b8-6d90-494c-99dd-ddd5ffaa90a8",
   "metadata": {},
   "source": [
    "## Task 1. Locally Weighted Linear Regression [6 points]\n",
    "\n",
    "Under the assumption that $\\mathbf{X}^\\top W(\\mathbf{x}_0) \\mathbf{X}$ is inverible, derive the closed form solution for the LWR problem, defined in Task 3 of the practical part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c73ad6eab0ad3b",
   "metadata": {},
   "source": [
    "### Your solution:\n",
    "To derive the closed form solution for the LWR problem, we start from the objective function of Locally Weighted Regression which is \n",
    "\n",
    "$J(\\theta) = \\sum_{i=1}^{n} w_i(\\mathbf{x}_0) \\left(y_i - \\theta^\\top \\mathbf{x}_i\\right)^2$\n",
    "\n",
    "and the weight equation which is\n",
    "\n",
    "$w_i(\\mathbf{x}_0) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_0\\|^2}{2\\tau^2}\\right)$\n",
    "\n",
    "First we differentiate J(W) with respect to \\theta by applying the chain rule.\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta} = -2 \\sum_{i=1}^{n} w_i(\\mathbf{x}_0) \\left(y_i - \\theta^\\top \\mathbf{x}_i\\right) \\frac{\\partial}{\\partial \\theta} \\left(y_i - \\theta^\\top \\mathbf{x}_i\\right) = -2 \\sum_{i=1}^{n} w_i(\\mathbf{x}_0) \\left(y_i - \\theta^\\top \\mathbf{x}_i\\right) \\left( -\\mathbf{x}_i^\\top \\right)$\n",
    "\n",
    "The derivative result should be set to 0, so\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta} = -2 \\sum_{i=1}^{n} w_i(\\mathbf{x}_0) \\left( y_i - \\theta^\\top \\mathbf{x}_i \\right) \\mathbf{x}_i = 0$\n",
    "\n",
    "Then when we split the summation into two components and send one of them to the left, we get: \n",
    "\n",
    "$\\sum_{i=1}^{n} w_i(\\mathbf{x}_0) (\\theta^\\top \\mathbf{x}_i) \\mathbf{x}_i = \\sum_{i=1}^{n} w_i(\\mathbf{x}_0) y_i \\mathbf{x}_i$\n",
    "\n",
    "Now we solve the formula that we gained for $\\( \\theta(\\mathbf{x}_0) \\)$:\n",
    "   \n",
    "$\\theta(\\mathbf{x}_0) = \\left(\\sum_{i=1}^{n} w_i(\\mathbf{x}_0) \\mathbf{x}_i \\mathbf{x}_i^\\top \\right)^{-1} \\sum_{i=1}^{n} w_i(\\mathbf{x}_0) y_i \\mathbf{x}_i$\n",
    "\n",
    "Multiplying these two terms calculates the weighted regression coefficients, where the first component calculates the inverse of the weighted sum of outer products, and the second component represents the weighted sum of the product of the target values y and feature vector x. \n",
    "\n",
    "Finally, we plug this expression back into the regression function:\n",
    "\n",
    "$\\hat{y}_0 = \\theta^\\top \\mathbf{x}_0 \\left( \\sum_{i=1}^{n} w_i(\\mathbf{x}_0) (\\theta^\\top \\mathbf{x}_i) \\mathbf{x}_i \\right)^{-1} \\sum_{i=1}^{n} w_i(\\mathbf{x}_0) y_i \\mathbf{x}_i$\n",
    "\n",
    "and finally we get: \n",
    "\n",
    "$\\hat{y}_0 = \\mathbf{x}_0^\\top (\\mathbf{X}^\\top W(\\mathbf{x}_0) \\mathbf{X})^{-1} \\mathbf{X}^\\top W(\\mathbf{x}_0) \\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2a8c7",
   "metadata": {},
   "source": [
    "## Task 2. Multiclass Naive Bayes Classifier [4 points]\n",
    "\n",
    "Let us consider **multiclass classification problem** with classes $C_1, \\dots, C_K$.\n",
    "\n",
    "Assume that all $d$ features $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_d \\end{bmatrix}$ are **binary**, i.e. $x_{i} \\in \\{0, 1\\}$ for $i = \\overline{1, d}$ **or** feature vector $\\mathbf{x} \\in \\{0, 1\\}^d$.\n",
    "\n",
    "Show that the decision rule of a **Naive Bayes Classifier** can be represented as $\\arg\\max$ of linear functions of the input.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Hint**: use the **maximum a posteriori** (MAP) decision rule: $\\hat{y} = \\arg\\max\\limits_{y \\in \\overline{1, K}} p(y)p(\\mathbf{x}|y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372e710",
   "metadata": {},
   "source": [
    "### Your solution:\n",
    "\n",
    "Naive Bayes classifier assumes that the features are conditionally independent events, so for events of class $\\(C_k\\)$ the likelihood can be written as the product of the individual feature probabilities.\n",
    "\n",
    "$p(\\mathbf{x}|y=C_k) = p(x_1|y=C_k) \\times p(x_2|y=C_k) \\times \\ldots \\times p(x_d|y=C_k)$\n",
    "\n",
    "Assuming that all d features of vector x are binary $(\\(x_i \\in \\{0, 1\\}\\)), \\(p(x_i|y=C_k)\\)$ represents the probability that feature $\\(x_i\\)$ takes the value 1 given the class $\\(C_k\\)$.\n",
    "\n",
    "Using the maximum a posteriori (MAP) decision rule as suggested, we find the class $\\(C_k\\)$ that maximizes the posterior probability, where using Bayes Theorem on conditional independence, we have:\n",
    "\n",
    "$p(y=C_k|\\mathbf{x}) = \\frac{p(y=C_k)p(\\mathbf{x}|y=C_k)}{p(\\mathbf{x})}$\n",
    "\n",
    "We consider the constant $p(\\mathbf{x})$ so we simplify the maximizing formula to the one below\n",
    "\n",
    "$\\hat{y} = \\arg\\max\\limits_{k} p(y=C_k)p(\\mathbf{x}|y=C_k)$.\n",
    "\n",
    "Because the conditionally independent events can be expressed as the product between their individual probabilities, we have:\n",
    "\n",
    "$p(\\mathbf{x}|y=C_k) = p(x_1|y=C_k) \\times p(x_2|y=C_k) \\times \\ldots \\times p(x_d|y=C_k)$ where every component $\\(p(x_i|y=C_k)\\)$ can be treated as a linear function of the input feature $\\(x_i\\)$. Considering that we have a binary case, $\\(p(x_i=1|y=C_k)\\)$ represents the weight associated with feature $\\(x_i\\)$, and $\\(p(x_i=0|y=C_k)\\)$ can be considered as $\\(1 - \\)$ the weight associated with feature $\\(x_i\\)$ - keeping in mind that the total probability is 1.\n",
    "\n",
    "Therefore, the decision rule $\\(\\hat{y} = \\arg\\max\\limits_{k} p(y=C_k)p(\\mathbf{x}|y=C_k)\\)$ can be represented as the $\\(\\arg\\max\\)$ of linear functions of the input features:\n",
    "\n",
    "$\\hat{y} = \\arg\\max\\limits_{k} \\left[ p(y=C_k) \\times \\left( \\sum_{i=1}^{d} w_i^{(k)}x_i + (1 - w_i^{(k)})(1 - x_i) \\right) \\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50865f987831f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T21:33:00.215902Z",
     "start_time": "2023-10-31T21:33:00.202778Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
